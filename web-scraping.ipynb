{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (CSIS 44-620)\n",
    "\n",
    "## P6: Web Scraping, NLP (Requests, BeautifulSoup, and spaCy) & Engage\n",
    "\n",
    "### \n",
    "Author: Data-Git-Hub <br>\n",
    "GitHub Project Repository Link: https://github.com/Data-Git-Hub/web-scraping <br>\n",
    "6 July 2025 <br>\n",
    "\n",
    "### Introduction\n",
    "In this project, I explore the fundamentals of web scraping and natural language processing (NLP) using Python within a Jupyter Notebook environment. The primary objective is to extract textual data from web sources and perform basic NLP tasks to analyze and interpret that data. This includes using libraries such as `requests` to fetch web content, `BeautifulSoup` to parse HTML, and `spaCy` with `spacytextblob` to process and analyze text for sentiment, subjectivity, and linguistic patterns. <br>\n",
    "\n",
    "Web scraping is an essential skill in data analytics and business intelligence, enabling analysts to gather real-time or hard-to-find data from public web pages. NLP extends this capability by allowing us to interpret the collected text, uncover hidden insights, and support data-driven decision-making. Together, these skills allow for scalable and automated information extraction that can inform strategy, research, and communication analysis. <br>\n",
    "\n",
    "This project also demonstrates effective use of Python virtual environments, version control with GitHub, and professional documentation practices. All code has been executed prior to submission, and final versions have been exported to HTML to ensure accessibility and review readiness. The final submission includes code, outputs, visualizations, and reflections on the process. <br>\n",
    "\n",
    "### Imports\n",
    "Python libraries are collections of pre-written code that provide specific functionalities, making programming more efficient and reducing the need to write code from scratch. These libraries cover a wide range of applications, including data analysis, machine learning, web development, and automation. Some libraries, such as os, sys, math, json, and datetime, come built-in with Python as part of its standard library, providing essential functions for file handling, system operations, mathematical computations, and data serialization. Other popular third-party libraries, like `pandas`, `numpy`, `matplotlib`, `seaborn`, and `scikit-learn`, must be installed separately and are widely used in data science and machine learning. The extensive availability of libraries in Python's ecosystem makes it a versatile and powerful programming language for various domains. <br>\n",
    "\n",
    "`beautifulsoup4` is a Python library used for parsing HTML and XML documents. It provides Pythonic methods for navigating, searching, and modifying the parse tree, making it ideal for web scraping tasks. BeautifulSoup is particularly useful for extracting data from web pages with inconsistent or poorly structured HTML. It works well with parsers like `html5lib` and `lxml`. <br>\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/ <br>\n",
    "\n",
    "`html5lib` is a pure-Python HTML parser designed to parse documents the same way modern web browsers do. It is especially useful for handling malformed or messy HTML. When used with `beautifulsoup4`, it provides robust parsing capabilities that help ensure accurate and tolerant extraction of web content. <br>\n",
    "https://html5lib.readthedocs.io/en/latest/ <br>\n",
    "\n",
    "`ipykernel` allows Jupyter Notebooks to run Python code by providing the kernel interface used to execute cells and handle communication between the front-end and the Python interpreter. <br>\n",
    "https://ipykernel.readthedocs.io/en/latest/ <br>\n",
    "\n",
    "`jupyterlab` is the next-generation user interface for Project Jupyter. It offers a flexible, extensible environment for interactive computing with support for code, markdown, visualizations, and terminals all within a tabbed workspace. JupyterLab enhances productivity by allowing users to organize notebooks, text editors, and data file viewers side by side. <br>\n",
    "https://jupyterlab.readthedocs.io/en/stable/ <br>\n",
    "\n",
    "`Matplotlib` is a widely used data visualization library that allows users to create static, animated, and interactive plots. It provides extensive tools for generating various chart types, including line plots, scatter plots, histograms, and bar charts, making it a critical library for exploratory data analysis. <br>\n",
    "https://matplotlib.org/stable/contents.html <br>\n",
    "\n",
    "`notebook` is the Python package that powers the classic Jupyter Notebook interface. It provides a web-based environment for writing and running code in interactive cells, supporting rich media, visualizations, and markdown documentation. The notebook server manages the execution of kernels and renders notebooks in a browser. This tool is foundational for data analysis, teaching, and exploratory programming workflows. <br>\n",
    "https://jupyter-notebook.readthedocs.io/en/stable/ <br>\n",
    "\n",
    "`Pandas` is a powerful data manipulation and analysis library that provides flexible data structures, such as DataFrames and Series. It is widely used for handling structured datasets, enabling easy data cleaning, transformation, and aggregation. Pandas is essential for data preprocessing in machine learning and statistical analysis. <br>\n",
    "https://pandas.pydata.org/docs/ <br>\n",
    "\n",
    "The `requests` library simplifies making HTTP requests in Python, allowing you to send GET, POST, and other types of requests to interact with APIs or web services. <br>\n",
    " https://docs.python-requests.org/en/latest/ <br>\n",
    "\n",
    "`spaCy` is an advanced NLP library for Python that provides tools for tokenization, part-of-speech tagging, named entity recognition, and more, using pre-trained pipelines. <br>\n",
    "https://spacy.io/ <br>\n",
    "\n",
    "`spacytextblob` is a plugin for spaCy that adds sentiment analysis capabilities by integrating TextBlob's polarity and subjectivity scores into spaCy’s pipeline. <br>\n",
    "https://github.com/AndrewIbrahim/spacy-textblob <br>\n",
    "\n",
    "`TextBlob` is a Python library for processing textual data, built on top of `nltk` and `pattern`. It provides a simple API for common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, translation, and sentiment analysis. Its intuitive design and built-in sentiment scoring functions make it especially useful for quick prototyping and educational applications. <br>\n",
    "https://textblob.readthedocs.io/en/dev/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository. <br>\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1. Extract and Save Article HTML\n",
    "Write code that extracts the article html from https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/ and dumps it to a .pkl (or other appropriate file) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content successfully saved to C:/Projects/web-scraping/dump\\article_html.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define URL and local path\n",
    "url = \"https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/\"\n",
    "dump_folder = \"C:/Projects/web-scraping/dump\"\n",
    "os.makedirs(dump_folder, exist_ok=True)\n",
    "\n",
    "# Get HTML content from the URL\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Define path to save .pkl file\n",
    "pkl_path = os.path.join(dump_folder, \"article_html.pkl\")\n",
    "\n",
    "# Dump HTML to .pkl file\n",
    "with open(pkl_path, \"wb\") as file:\n",
    "    pickle.dump(html_content, file)\n",
    "\n",
    "print(f\"HTML content successfully saved to {pkl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2. Load and Display Article Text\n",
    "Read in your article's html source from the file you created in question 1 and print it's text (use `.get_text()`) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Laser Headlights Work | Hackaday Skip to content Hackaday Primary Menu Home Blog Hackaday.io Tindie Hackaday Prize Submit About Search for: March 27, 2021 How Laser Headlights Work 130 Comments by: Lewin Day March 22, 2021 When we think about the onward march of automotive technology, headlights aren’t usually the first thing that come to mind. Engines, fuel efficiency, and the switch to electric power are all more front of mind. However, that doesn’t mean there aren’t thousands of engineers around the world working to improve the state of the art in automotive lighting day in, day out. Sealed beam headlights gave way to more modern designs once regulations loosened up, while bulbs moved from simple halogens to xenon HIDs and, more recently, LEDs. Now, a new technology is on the scene, with lasers! Laser Headlights?! BWM’s prototype laser headlight assemblies undergoing testing. The first image brought to mind by the phrase “laser headlights” is that of laser beams firing out the front of an automobile. Obviously, coherent beams of monochromatic light would make for poor illumination outside of a very specific spot quite some distance away. Thankfully for our eyes, laser headlights don’t work in this way at all. Instead, laser headlights consist of one or more solid state laser diodes mounted inside the headlight. These blue lasers are fired at a yellow phosphor, similar to that used in white LEDs. This produces a powerful, vibrant white light that can then be bounced off\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "pkl_path = \"C:/Projects/web-scraping/dump/article_html.pkl\"\n",
    "parse_folder = \"C:/Projects/web-scraping/parse\"\n",
    "os.makedirs(parse_folder, exist_ok=True)\n",
    "txt_path = os.path.join(parse_folder, \"article_text.txt\")\n",
    "\n",
    "# Load HTML content\n",
    "with open(pkl_path, \"rb\") as file:\n",
    "    html_content = pickle.load(file)\n",
    "\n",
    "# Parse and extract text\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "raw_text = soup.get_text()\n",
    "\n",
    "# Normalize whitespace to a single space\n",
    "cleaned_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "\n",
    "# Save cleaned text to .txt\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "# Print first 1500 characters\n",
    "print(cleaned_text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3. Analyze Most Frequent Tokens with spaCy\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 10 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Tokens (excluding stopwords, punctuation, and whitespace):\n",
      "\n",
      "Token: 'comment' | Frequency: 136\n",
      "Token: 'march' | Frequency: 133\n",
      "Token: '2021' | Frequency: 133\n",
      "Token: 'says' | Frequency: 132\n",
      "Token: 'report' | Frequency: 130\n",
      "Token: 'reply' | Frequency: 112\n",
      "Token: '22' | Frequency: 79\n",
      "Token: 'hacks' | Frequency: 76\n",
      "Token: 'light' | Frequency: 68\n",
      "Token: 'laser' | Frequency: 63\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the article text\n",
    "txt_path = \"C:/Projects/web-scraping/parse/article_text.txt\"\n",
    "with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    article_text = file.read()\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Filter tokens: no punctuation, no stop words, no whitespace\n",
    "tokens = [\n",
    "    token.text.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "# Count token frequencies\n",
    "token_freq = Counter(tokens)\n",
    "most_common_tokens = token_freq.most_common(10)\n",
    "\n",
    "# Print results\n",
    "print(\"Top 10 Most Frequent Tokens (excluding stopwords, punctuation, and whitespace):\\n\")\n",
    "for token, freq in most_common_tokens:\n",
    "    print(f\"Token: '{token}' | Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4. Analyze Most Frequent Lemmas with spaCy\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 10 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Lemmas (excluding stopwords, punctuation, and whitespace):\n",
      "\n",
      "Lemma: 'comment' | Frequency: 157\n",
      "Lemma: 'say' | Frequency: 134\n",
      "Lemma: 'march' | Frequency: 133\n",
      "Lemma: '2021' | Frequency: 133\n",
      "Lemma: 'report' | Frequency: 130\n",
      "Lemma: 'reply' | Frequency: 112\n",
      "Lemma: 'light' | Frequency: 99\n",
      "Lemma: '22' | Frequency: 79\n",
      "Lemma: 'headlight' | Frequency: 74\n",
      "Lemma: 'laser' | Frequency: 72\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model (ensure it's installed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the article text\n",
    "txt_path = \"C:/Projects/web-scraping/parse/article_text.txt\"\n",
    "with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    article_text = file.read()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Extract cleaned lemmas (lowercased), filtering out stopwords, punctuation, and whitespace\n",
    "lemmas = [\n",
    "    token.lemma_.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "# Count lemma frequencies\n",
    "lemma_freq = Counter(lemmas)\n",
    "most_common_lemmas = lemma_freq.most_common(10)\n",
    "\n",
    "# Print results\n",
    "print(\"Top 10 Most Frequent Lemmas (excluding stopwords, punctuation, and whitespace):\\n\")\n",
    "for lemma, freq in most_common_lemmas:\n",
    "    print(f\"Lemma: '{lemma}' | Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4.1. Comparison Between Tokens and Lemmas\n",
    "When comparing the most frequent tokens to their corresponding lemmas, it becomes clear that while the two lists often overlap, they serve different purposes in text analysis. Tokens are the exact word forms as they appear in the text, preserving tense, plurality, and other variations. Lemmas, on the other hand, represent the base or dictionary form of each word, effectively grouping similar forms under a single root. For instance, the token \"says\" appears frequently, but in the lemmatized list it is transformed into \"say\", which may include both \"says\" and other forms like \"said\" or \"saying\" if present. This results in slight frequency differences—lemmas often have higher counts due to this aggregation. Furthermore, some words like \"comment\", \"march\", or \"laser\" appear identically in both token and lemma form, showing no morphological change. Ultimately, token analysis is useful for understanding surface-level patterns, while lemmatization provides a more generalized linguistic view that reduces redundancy in vocabulary analysis. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 5. Score Sentences by Token and Lemma Frequency\n",
    "\n",
    "Define the following methods:\n",
    "    * `score_sentence_by_token(sentence, interesting_token)` that takes a sentence and a list of interesting token and returns the number of times that any of the interesting words appear in the sentence divided by the number of words in the sentence <br>\n",
    "    * `score_sentence_by_lemma(sentence, interesting_lemmas)` that takes a sentence and a list of interesting lemmas and returns the number of times that any of the interesting lemmas appear in the sentence divided by the number of words in the sentence <br>\n",
    "    \n",
    "You may find some of the code from the in class notes useful; feel free to use methods (rewrite them in this cell as well).  Test them by showing the score of the first sentence in your article using the frequent tokens and frequent lemmas identified in question 3. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: How Laser Headlights Work | Hackaday Skip to content Hackaday Primary Menu Home Blog Hackaday.io Tindie Hackaday Prize Submit About Search for: March 27, 2021 How Laser Headlights Work 130 Comments by: Lewin Day March 22, 2021 When we think about the onward march of automotive technology, headlights aren’t usually the first thing that come to mind.\n",
      "\n",
      "Token Score: 0.1379\n",
      "Lemma Score: 0.1724\n"
     ]
    }
   ],
   "source": [
    "# Reuse the spaCy doc from earlier\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Get first sentence from the article\n",
    "sentences = list(doc.sents)\n",
    "first_sentence = sentences[0]\n",
    "\n",
    "# Frequent tokens and lemmas from Sections 3 and 4\n",
    "# Replace with your actual most_common output if different\n",
    "frequent_tokens = ['comment', 'march', '2021', 'says', 'report', 'reply', '22', 'hacks', 'light', 'laser']\n",
    "frequent_lemmas = ['comment', 'say', 'march', '2021', 'report', 'reply', 'light', '22', 'headlight', 'laser']\n",
    "\n",
    "# Define scoring function using tokens\n",
    "def score_sentence_by_token(sentence, interesting_tokens: List[str]) -> float:\n",
    "    tokens = [\n",
    "        token.text.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    matches = [token for token in tokens if token in interesting_tokens]\n",
    "    return len(matches) / len(tokens)\n",
    "\n",
    "# Define scoring function using lemmas\n",
    "def score_sentence_by_lemma(sentence, interesting_lemmas: List[str]) -> float:\n",
    "    lemmas = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    if not lemmas:\n",
    "        return 0.0\n",
    "    matches = [lemma for lemma in lemmas if lemma in interesting_lemmas]\n",
    "    return len(matches) / len(lemmas)\n",
    "\n",
    "# Score the first sentence\n",
    "token_score = score_sentence_by_token(first_sentence, frequent_tokens)\n",
    "lemma_score = score_sentence_by_lemma(first_sentence, frequent_lemmas)\n",
    "\n",
    "print(f\"First sentence: {first_sentence.text.strip()}\\n\")\n",
    "print(f\"Token Score: {token_score:.4f}\")\n",
    "print(f\"Lemma Score: {lemma_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 5.1. Interpretation of Sentence Scoring\n",
    "The first sentence of the article contains a mix of metadata, navigation text, and introductory content. Despite its length, only a small portion of the words relate directly to the most frequent concepts in the article. The token score of 0.1379 means that approximately 13.8% of the individual word forms (tokens) in the sentence match the list of \"interesting\" frequent tokens identified earlier (e.g., \"comment,\" \"march,\" \"2021\"). The lemma score of 0.1724 is slightly higher, indicating that about 17.2% of the root word forms (lemmas) in the sentence match the frequent lemmas. This small difference arises because the lemmatization process aggregates variations of words—like \"says\" and \"say\"—which allows for slightly broader matching. Overall, these scores suggest that while the sentence includes some important contextual keywords, much of the text is unrelated interface or structural content from the website, reducing its relevance to the core topic of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 6. \n",
    "Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 7. \n",
    "Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 8. \n",
    "Which tokens and lexems would be ommitted from the lists generated in questions 3 and 4 if we only wanted to consider nouns as interesting words?  How might we change the code to only consider nouns? Put your answer in this Markdown cell (you can edit it by double clicking it). <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
